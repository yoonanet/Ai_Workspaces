{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a38b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전 실습은 별도의 은닉층없이 신경망을 구성해서 확인을 해본 것이다.\n",
    "#우리가 신경망을 딥하게 가져가면 (은닉층을 다양하게 구성) 좀 더 정확도 높은 모델이 생성이 되어질 것임\n",
    "#항상 딥하게 은닉층을 가져간다고 해서 100% 좋은 정확도를 가지는 것은 아님 따라서 실습을 통해 확인해보도록 할 것임.\n",
    "\n",
    "from tensorflow import keras #keras로 다이렉트 접근이 가능함\n",
    "\n",
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ac2a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, train_target.shape, test_input.shape, test_target.shape) #데이터가 제대로 담겨져 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a13e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#훈련용 데이터에서 검증용데이터를 따로 뽑아내도록 한다.\n",
    "#먼저, 데이터를 정규화해주도록 한다.\n",
    "train_scaled = train_input / 255.\n",
    "train_scaled = train_scaled.reshape(-1, 28*28) #1차원으로 펼쳐주면서 입력데이터로 활용하도록 한다.\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=1234)\n",
    "#훈련데이터를 8:2(훈련과 검증)로 분리해서 인덱스 값으로 반환해주게 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b063d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (48000,) (12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_scaled.shape, train_target.shape, val_scaled.shape, val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3734aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 만들기\n",
    "# layers(층)을 추가하는 방법1\n",
    "dense1 = keras.layers.Dense(units=100, input_shape=(28 * 28,), activation='sigmoid') #첫번째 은닉층의 뉴런의 갯수 100개로 출력\n",
    "#최종 출력단만 softmax로 지정을 해주면 된다. / 그전까지의 각각의 은닉층은 sigmoid로 지정\n",
    "dense2 = keras.layers.Dense(units=10, activation='softmax') #추가적인 은닉층, 바로 출력단으로 연결 (1개만 추가하도록 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([dense1, dense2]) #기존에는 model에 add하면서 추가를 해줬었는데 리스트를 통해서 신경망에 세포들을 넣어주도록 한다.\n",
    "\n",
    "model.summary()\n",
    "#Param = 784(28*28) * 100(출력) + 100(바이어스값) = 78500\n",
    "# 78400이 W의 값이고 바이어스계층이 100개임\n",
    "#이를 출력단에 연결!! 출력이 10개일때 w의 갯수는 100 * 10 + 바이어스값 10개 = 1010\n",
    "\n",
    "#단지 은닉층 하나만 추가했을뿐인데 79,400으로 독립적인 w의 갯수를 각각 현재 입력데이터에 대해서 계산을 해주게 되는 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e8c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers(층)을 추가하는 방법2 - 다이렉트로 은닉층을 넣어줄 수 있음. +) dense마다 이름을 부여해줄 수 있음\n",
    "\n",
    "#model = keras.Sequential([ #신경망 생성과 동시에 dense를 다이렉트로 전달해줄 수 있음\n",
    "#    keras.layers.Dense(units=100, input_shape=(28 * 28,), activation='sigmoid', name='hidden'), # 은닉층이라는 의미 hidden의 이름 부여\n",
    "#    keras.layers.Dense(units=10, activation='softmax', name='output') #출력단이라는 이름을 부여\n",
    "#], name='패션 MNIST 모델') \n",
    "\n",
    "#model.summary() #모델에 대한 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d738f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd', metrics='accuracy')\n",
    "# 데이터값 자체가 정수값으로 담겨져 있으면 그것을 원핫인코딩으로 펼쳐줘야 하는 것임\n",
    "# 원핫인코딩을 함수를 이용해서 펼쳐주는게 귀찮다면 sparse_의 키워드를 붙여주면서 자동적으로 펼쳐주도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e297faff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3804 - accuracy: 0.6552\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8414 - accuracy: 0.7459\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7068 - accuracy: 0.7672\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6404 - accuracy: 0.7829\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5977 - accuracy: 0.7965\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5671 - accuracy: 0.8050\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5436 - accuracy: 0.8135\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5256 - accuracy: 0.8185\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5111 - accuracy: 0.8222\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4989 - accuracy: 0.8268\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4889 - accuracy: 0.8310\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4799 - accuracy: 0.8334\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4724 - accuracy: 0.8358\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4652 - accuracy: 0.8388\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4594 - accuracy: 0.8398\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4537 - accuracy: 0.8420\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4489 - accuracy: 0.8431\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4441 - accuracy: 0.8447\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4398 - accuracy: 0.8459\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4356 - accuracy: 0.8477\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4320 - accuracy: 0.8489\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4285 - accuracy: 0.8494\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4250 - accuracy: 0.8508\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4218 - accuracy: 0.8519\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4188 - accuracy: 0.8528\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4162 - accuracy: 0.8531\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4130 - accuracy: 0.8549\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4106 - accuracy: 0.8562\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4081 - accuracy: 0.8565\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4058 - accuracy: 0.8574\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4033 - accuracy: 0.8582\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4013 - accuracy: 0.8586\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3988 - accuracy: 0.8598\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3970 - accuracy: 0.8604\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3949 - accuracy: 0.8607\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3929 - accuracy: 0.8613\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3913 - accuracy: 0.8623\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3893 - accuracy: 0.8629\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3875 - accuracy: 0.8637\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3856 - accuracy: 0.8636\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3843 - accuracy: 0.8642\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3826 - accuracy: 0.8653\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3811 - accuracy: 0.8659\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3793 - accuracy: 0.8662\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3781 - accuracy: 0.8669\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3765 - accuracy: 0.8677\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3747 - accuracy: 0.8677\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3736 - accuracy: 0.8694\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3722 - accuracy: 0.8688\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3706 - accuracy: 0.8693\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3694 - accuracy: 0.8698\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3680 - accuracy: 0.8704\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3670 - accuracy: 0.8710\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3657 - accuracy: 0.8713\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3643 - accuracy: 0.8723\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3631 - accuracy: 0.8718\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3620 - accuracy: 0.8733\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3610 - accuracy: 0.8725\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3594 - accuracy: 0.8729\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3585 - accuracy: 0.8739\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3575 - accuracy: 0.8749\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3565 - accuracy: 0.8749\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3553 - accuracy: 0.8750\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3541 - accuracy: 0.8753\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3531 - accuracy: 0.8752\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3521 - accuracy: 0.8764\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3511 - accuracy: 0.8767\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3500 - accuracy: 0.8763\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3489 - accuracy: 0.8768\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3481 - accuracy: 0.8773\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3469 - accuracy: 0.8780\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3461 - accuracy: 0.8784\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3451 - accuracy: 0.8785\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3441 - accuracy: 0.8792\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3433 - accuracy: 0.8784\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3423 - accuracy: 0.8806\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3414 - accuracy: 0.8786\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3405 - accuracy: 0.8793\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3397 - accuracy: 0.8807\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3388 - accuracy: 0.8804\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3378 - accuracy: 0.8808\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3371 - accuracy: 0.8811\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3361 - accuracy: 0.8818\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3354 - accuracy: 0.8820\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3344 - accuracy: 0.8821\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3337 - accuracy: 0.8817\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3326 - accuracy: 0.8822\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3322 - accuracy: 0.8833\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3311 - accuracy: 0.8834\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3304 - accuracy: 0.8829\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3296 - accuracy: 0.8834\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3288 - accuracy: 0.8849\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3282 - accuracy: 0.8844\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3273 - accuracy: 0.8842\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3265 - accuracy: 0.8848\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3256 - accuracy: 0.8849\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3250 - accuracy: 0.8854\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3240 - accuracy: 0.8858\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3236 - accuracy: 0.8858\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3229 - accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d19321970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_scaled, train_target, epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea5b227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3638 - accuracy: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3638346493244171, 0.8681666851043701]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target) #검증용으로 뽑아둔 데이터를 통해 정확도를 확인\n",
    "#단지 은닉층을 하나 추가한 것 뿐인데도 거의 2%의 정확도가 높아진 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5a2fb",
   "metadata": {},
   "source": [
    "# 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(momentum=0.9) #모멘텀도 튜닝의 값으로 최적의 값을 찾아가야 함.\n",
    "\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy') #우리가 모멘텀을 지정할 적용값으로 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98054c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3400 - accuracy: 0.8793\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3333 - accuracy: 0.8802\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3267 - accuracy: 0.8836\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3200 - accuracy: 0.8849\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3179 - accuracy: 0.8847\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3103 - accuracy: 0.8881\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3044 - accuracy: 0.8899\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2994 - accuracy: 0.8919\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2954 - accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2899 - accuracy: 0.8952\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2875 - accuracy: 0.8964\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2811 - accuracy: 0.8979\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2786 - accuracy: 0.8983\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2733 - accuracy: 0.9020\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2709 - accuracy: 0.9025\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2672 - accuracy: 0.9035\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2639 - accuracy: 0.9053\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2605 - accuracy: 0.9055\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2575 - accuracy: 0.9071\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2542 - accuracy: 0.9084\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2509 - accuracy: 0.9103\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2485 - accuracy: 0.9106\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2446 - accuracy: 0.9123\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2430 - accuracy: 0.9118\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2392 - accuracy: 0.9144\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2358 - accuracy: 0.9144\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2348 - accuracy: 0.9162\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2328 - accuracy: 0.9150\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2295 - accuracy: 0.9174\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2263 - accuracy: 0.9176\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2240 - accuracy: 0.9190\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2211 - accuracy: 0.9204\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9202\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2163 - accuracy: 0.9217\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2151 - accuracy: 0.9233\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2117 - accuracy: 0.9223\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2104 - accuracy: 0.9231\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2089 - accuracy: 0.9251\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2057 - accuracy: 0.9260\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2032 - accuracy: 0.9267\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2001 - accuracy: 0.9287\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1993 - accuracy: 0.9292\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1976 - accuracy: 0.9285\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1958 - accuracy: 0.9298\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1930 - accuracy: 0.9310\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1906 - accuracy: 0.9310\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1890 - accuracy: 0.9328\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1878 - accuracy: 0.9325\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1858 - accuracy: 0.9331\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1845 - accuracy: 0.9340\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1815 - accuracy: 0.9346\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1790 - accuracy: 0.9361\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1772 - accuracy: 0.9370\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1764 - accuracy: 0.9370\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1738 - accuracy: 0.9383\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1736 - accuracy: 0.9380\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.9404\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1676 - accuracy: 0.9409\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1672 - accuracy: 0.9402\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1652 - accuracy: 0.9413\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1642 - accuracy: 0.9424\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1624 - accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1603 - accuracy: 0.9436\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1606 - accuracy: 0.9430\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1582 - accuracy: 0.9441\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1557 - accuracy: 0.9452\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1536 - accuracy: 0.9461\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1517 - accuracy: 0.9466\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.9467\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1520 - accuracy: 0.9454\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.9483\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1465 - accuracy: 0.9492\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1454 - accuracy: 0.9496\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1438 - accuracy: 0.9492\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1419 - accuracy: 0.9503\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1424 - accuracy: 0.9496\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1392 - accuracy: 0.9514\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1379 - accuracy: 0.9517\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1378 - accuracy: 0.9523\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1350 - accuracy: 0.9534\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1338 - accuracy: 0.9539\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1323 - accuracy: 0.9534\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1312 - accuracy: 0.9550\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1300 - accuracy: 0.9544\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1298 - accuracy: 0.9551\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1281 - accuracy: 0.9553\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1277 - accuracy: 0.9564\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1247 - accuracy: 0.9564\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1237 - accuracy: 0.9582\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1231 - accuracy: 0.9570\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1233 - accuracy: 0.9571\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1208 - accuracy: 0.9585\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1203 - accuracy: 0.9582\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.9599\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1193 - accuracy: 0.9592\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.9610\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.9610\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.9624\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.9618\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.9624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d20b53fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_scaled, train_target, epochs=100)\n",
    "#기존에 했었던 정확도 값보다 높았던 것을 봐서 생각해보면 변곡점을 찾았던 것이라고 예측할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e56f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 957us/step - loss: 0.3704 - accuracy: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37035199999809265, 0.8871666789054871]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target) #오버피팅이 된 것이 아닌지를 확인하기 위함\n",
    "# 학습에 대한 정확도는 96%, 검증의 정확도가 88%가 나왔다는 것은 오버피팅의 가능성이 높다는 것임.\n",
    "# 하지만 기존에 비해 3%가 높아졌다는 것은 가속의 물리적인 업데이트가 이루어지면서 개선되어지는 효과를 보여준 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcac10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0952 - accuracy: 0.9710\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0914 - accuracy: 0.9732\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0909 - accuracy: 0.9733\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0905 - accuracy: 0.9734\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0904 - accuracy: 0.9733\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0903 - accuracy: 0.9734\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0902 - accuracy: 0.9735\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0901 - accuracy: 0.9736\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0900 - accuracy: 0.9736\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0899 - accuracy: 0.9739\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0899 - accuracy: 0.9737\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0899 - accuracy: 0.9736\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0898 - accuracy: 0.9738\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0897 - accuracy: 0.9739\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0897 - accuracy: 0.9738\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0897 - accuracy: 0.9737\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0896 - accuracy: 0.9740\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0896 - accuracy: 0.9738\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0896 - accuracy: 0.9740\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0895 - accuracy: 0.9742\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0895 - accuracy: 0.9740\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0895 - accuracy: 0.9742\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9739\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9739\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9739\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9741\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9741\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0893 - accuracy: 0.9741\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0893 - accuracy: 0.9742\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0893 - accuracy: 0.9742\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0892 - accuracy: 0.9743\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0892 - accuracy: 0.9743\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0892 - accuracy: 0.9742\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0892 - accuracy: 0.9743\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9742\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9746\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9743\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9744\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9742\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0891 - accuracy: 0.9743\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0890 - accuracy: 0.9744\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0890 - accuracy: 0.9747\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0890 - accuracy: 0.9744\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0890 - accuracy: 0.9744\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9744\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9744\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9744\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9745\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9745\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0889 - accuracy: 0.9746\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9744\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9744\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9743\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9746\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9746\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9747\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0888 - accuracy: 0.9746\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9745\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9747\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9746\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9747\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.9747\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9746\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9746\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9746\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9746\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9746\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9747\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0886 - accuracy: 0.9748\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9747\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9747\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9747\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9745\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9745\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9747\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9747\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9747\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9747\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9747\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9749\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9745\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0884 - accuracy: 0.9747\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9749\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9749\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9747\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9748\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9747\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9747\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9746\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9748\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9748\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9748\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9749\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9749\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9747\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9747\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9748\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0881 - accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0882 - accuracy: 0.9749\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0881 - accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d20d4ad30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adagrad (학습률에 대한 가변 적용/ \n",
    "#          성능 개선의 목적을 두고 있는 것이 아니라 효율적으로 빠르게 \n",
    "#          학습레잇의 값을 가변시키면서 학습시키는 것에 포커스를 두고 있음)\n",
    "adagrad = keras.optimizers.Adagrad() #keras에서 클래스로 정의를 해주고 있음\n",
    "\n",
    "model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "model.fit(train_scaled, train_target, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f87511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3283 - accuracy: 0.8973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3282861113548279, 0.8973333239555359]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94ff8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#아담 알고리즘: 위에 두 개의 알고리즘 효과를 가져갈 수 있음\n",
    "model = keras.Sequential()\n",
    "\n",
    "#model.add(keras.layers.Flatten(input_shape=(28, 28))) 'sparse_' 키워드를 통해 정수값자체로 펼쳐지기 때문에 이 코드는 주석처리\n",
    "model.add(keras.layers.Dense(units=100, activation='relu')) #한개의 은닉층으로 구성\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "922fba71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5205 - accuracy: 0.8181\n",
      "Epoch 2/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3910 - accuracy: 0.8595\n",
      "Epoch 3/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3514 - accuracy: 0.8739\n",
      "Epoch 4/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3228 - accuracy: 0.8834\n",
      "Epoch 5/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3077 - accuracy: 0.8876\n",
      "Epoch 6/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2932 - accuracy: 0.8928\n",
      "Epoch 7/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2777 - accuracy: 0.8976\n",
      "Epoch 8/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2688 - accuracy: 0.9011\n",
      "Epoch 9/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2583 - accuracy: 0.9048\n",
      "Epoch 10/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2496 - accuracy: 0.9066\n",
      "Epoch 11/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2403 - accuracy: 0.9107\n",
      "Epoch 12/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2333 - accuracy: 0.9150\n",
      "Epoch 13/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2257 - accuracy: 0.9171\n",
      "Epoch 14/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2173 - accuracy: 0.9198\n",
      "Epoch 15/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2120 - accuracy: 0.9207\n",
      "Epoch 16/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2092 - accuracy: 0.9228\n",
      "Epoch 17/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2013 - accuracy: 0.9253\n",
      "Epoch 18/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1973 - accuracy: 0.9273\n",
      "Epoch 19/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1926 - accuracy: 0.9283\n",
      "Epoch 20/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1873 - accuracy: 0.9302\n",
      "Epoch 21/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1809 - accuracy: 0.9335\n",
      "Epoch 22/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1776 - accuracy: 0.9339\n",
      "Epoch 23/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1726 - accuracy: 0.9355\n",
      "Epoch 24/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1694 - accuracy: 0.9368\n",
      "Epoch 25/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1652 - accuracy: 0.9391\n",
      "Epoch 26/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1658 - accuracy: 0.9381\n",
      "Epoch 27/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1567 - accuracy: 0.9416\n",
      "Epoch 28/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1568 - accuracy: 0.9417\n",
      "Epoch 29/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1506 - accuracy: 0.9438\n",
      "Epoch 30/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1495 - accuracy: 0.9440\n",
      "Epoch 31/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1462 - accuracy: 0.9461\n",
      "Epoch 32/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1401 - accuracy: 0.9474\n",
      "Epoch 33/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1378 - accuracy: 0.9489\n",
      "Epoch 34/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1346 - accuracy: 0.9500\n",
      "Epoch 35/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1351 - accuracy: 0.9505\n",
      "Epoch 36/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1322 - accuracy: 0.9507\n",
      "Epoch 37/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1285 - accuracy: 0.9527\n",
      "Epoch 38/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1266 - accuracy: 0.9530\n",
      "Epoch 39/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1242 - accuracy: 0.9541\n",
      "Epoch 40/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1214 - accuracy: 0.9553\n",
      "Epoch 41/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1213 - accuracy: 0.9550\n",
      "Epoch 42/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.9568\n",
      "Epoch 43/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.9560\n",
      "Epoch 44/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.9599\n",
      "Epoch 45/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.9588\n",
      "Epoch 46/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.9605\n",
      "Epoch 47/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1063 - accuracy: 0.9605\n",
      "Epoch 48/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1040 - accuracy: 0.9621\n",
      "Epoch 49/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1020 - accuracy: 0.9628\n",
      "Epoch 50/50\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1011 - accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d2396f340>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adam = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "model.fit(train_scaled, train_target, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd431eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 959us/step - loss: 0.4648 - accuracy: 0.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4648056626319885, 0.890583336353302]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수까지의 개념을 살펴본 것임.\n",
    "#미니배치 학습 개념이 소개되고 있음 \n",
    "# => 별도로 셋팅을 하지 않으면 fit하여 학습을 진행할 때 미니학습으로 학습이 되어지게끔 디폴트처리가 되어지고 있음\n",
    "#빠른 결과를 피드백 받아볼 수 있음!!\n",
    "\n",
    "#미니배치의 개념 - 경사하강법에서 활용됨\n",
    "#패션 이미지 데이터셋은 실질적으로 학습을 할 때마다 6만개의 모든 데이터를 가지고 학습을 시킨다고 하면 시간이 어마어마하게 걸릴 수 있음\n",
    "#임의의 100장을 뽑아서 한 에포크마다 수행을 할 수 있게끔 한다.\n",
    "#한번 훈련에 적용되는 전체 이미지중에 학습에 사용할 뽑아낸 100장의 데이터를 미니배치라고 표현을 해준다.\n",
    "#훈련데이터 중에서 일부만 랜덤하게 골라서 학습할 때마다 에포크 수행시 처리되는 방법이다.\n",
    "\n",
    "#미니배치의 손실함수 계측을 통해서 전체 훈련데이터의 근사치를 이용\n",
    "#이렇게 학습을 시키더라도 전체 데이터로 학습시킨것과 별 차이가 없다는 것이 현재까지 검증된 바이다.\n",
    "\n",
    "#결과의 큰 차이가 없다고 한다면 학습시간을 단축시켜서 빠르게 모델을 생성할 수 있는 처리를 일반적으로 수행하게끔 하는 것임\n",
    "#따라서 디폴트도 미니배치를 수행하도록 텐서플로우나 사이킷런에 설정이 되어져 있음\n",
    "\n",
    "#손실함수를 통해서 w값에서 접선의 기울기를 구해서 접선의 기울기값에 기반해서 \n",
    "#다음 스탭을 이동해가면서 최종목표는 0에 근접하도록 하여 잔차의 값을 최소화하도록 한다. => 접선의 기울기를 구할 때 미분을 하면됨\n",
    "\n",
    "# 기울기라는 것은 두 점을 지나는 직선이다. 기울기는 어떻게 구할 수 있을까? x의 증가량분의 y의 증가량! \n",
    "# 그때의 x증가량을 h라고 한다면 x분의 f(x+h) - f(x)임 => 이에 대한 계산을 미분이라고 한다. 따라서 f(x)를 말함\n",
    "# h값을 0에 근접시키면 접선의 기울기가 나올 것이다. \n",
    "\n",
    "#두점이 지나가는 폭을 0에 가깝게 수렴시키면 접선의 기울기, 접선의 기울기가 미분의 정의가 됨\n",
    "\n",
    "\n",
    "#입력 핏쳐가 두 개를 가지는 x0, x1 데이터 셋에서 신경망을 구성한다고 할 때 \n",
    "#손실함수에서는 각 잔차에서의 오차값을 최소화하는 대표값으로 mse, crossentropy을 활용\n",
    "#다항식은 편미분(수치미분)을 이용해서 각각에 편미분에 의한 오차에 최소값을 찾아가는 수학적인 방법을 활용할 수 있다.\n",
    "# 편미분이란 x0를 기준으로 잡으면 그 나머지의 값들은 상수화되어지는 개념임\n",
    "# x0의 편미분을 구하라고 한다면 미분은 2x의 지수 -1을 해주면 됨. 상수를 미분하면 0이 됨. 편미분을 하게 되면 2x1임\n",
    "# 잔차의 최소의 값을 구하는 방법은 각각의 편미분에 의한 잔차의 값을 구하는 것이다.\n",
    "#문제발생) 그물망의 특징을 띄는 표를 위에서 바라보게 되면 가운데로 쏠리게 될 것임. \n",
    "#          이때, 방향을 가진 벡터의 형태로 그려지게 됨. 이때의 기울기는 함수에 가장 낮은 최소값을 가리키게 될 것이다.\n",
    "#                가장 낮은 곳에서 멀어질수록 화살표의 크기가 커지게 되는 특징을 가지게 됨\n",
    "#                기울기가 가리키는 곳은 출력의 값을 줄여가는 쪽으로 방향이 가져가게 됨\n",
    "\n",
    "#실제로 일일히 편미분을 구해서 w값을 업데이트하게끔 학습을 하느냐. 그렇지 않다.\n",
    "# 우리가 데이터를 저장할 때 중복을 허용하지 않는다면 저장하는 순간 기존 저장된 데이터인지를 먼저 검색을 해봐야함\n",
    "# 그러다보니 일일히 비교하는 방식으로 저장을 하게 된다면 문제없이 처리되어지지만 저장된 데이터의 양이 많아질수록 그만큼의 저장속도가 떨어짐\n",
    "# 효율적으로 데이터의 중복을 빠르게 처리할지에 대한 부분이 중복 데이터에 대한 허용을 하지 않는 기능에서의 핵심 아이디어, 알고리즘임\n",
    "# 3차원 공간을 벗어나는 특징이 다양하다면 한번에 보여주는 것이 불가능\n",
    "\n",
    "#=> 모든 변수의 편미분을 동시에 계산하고 싶다면 양쪽의 편미분을 묶어서 계산하도록 한다. 선형대수의 연산을 수행하면 됨\n",
    "\n",
    "#경사하강법 알고리즘을 통해서 적용을 해서 최종적인 잔차의 값이 최소화되어지는 순간에 최적의 w값을 찾아보는 것\n",
    "#경사하강법의 값은 어떻게 변화되는가. 현재 가중치값에서 한번에 학습이 일어난 순간에 미분을 구해보고 구한 미분의 러닝레잇을 빼줌\n",
    "# 접선의 기울기를 구하면서 얼마만큼으로 이동해가고, 지금 담겨진 w의 값을 업데이트 해주는 과정을 에폭시만큼 반복하면서 최상의 값을 뽑아내도록 한다.\n",
    "\n",
    "#신경망에서의 기울기: w가 2행 3열(망을 구성할 때 입력과 출력이 연결된 신경망이라고 가정)이라면 !!입력데이터는 두가지 핏쳐를 가지고 있고 출력은 3개로 되어진다고 생각할 수 있음!! \n",
    "#입력으로 학습하고 있는 데이터셋에 대해서 최적의 w값을 학습하는 것이 목표 -> 딥러닝\n",
    "\n",
    "#[확률적 경사 하강법] -> 데이터를 미니배치로 무작위로 선정하여 경사하강법으로 매개변수를 갱신(각각의 가중치(w)를 의미)\n",
    "# 신경망 같은 경우 학습절차: 가중치와 편향을 훈련데이터에 적응하도록 조정 -> 미니배치 -> 기울기 산출 -> 매개변수 갱신 -> 반복(잔차값이 더이상 줄어들지 않을 때까지)\n",
    "\n",
    "#오차역전파법 (해시의 개념을 적용한 내용임_검색을 빠르게 할 수 있음.)\n",
    "#실질적인 신경망에서의 그 수많은 각각의 w값들을 어떻게 학습이 이루어지면서 업데이트되어져 가는지를 실제 구현한 알고리즘임\n",
    "#내부에 손실함수와 경사하강법의 알고리즘이 실제로는 라이브러리안에 어떻게 구현이 되어져 있는지, 우리는 알고리즘을 들여다보면서 파악\n",
    "\n",
    "#계산그래프 : 연산기를 따로 빼놓고 분류 연산을 진행 (독립적으로 각각 계산하도록 함)\n",
    "# 오차역전파법이라는 알고리즘에 핵심은 지금부터 시작임 \n",
    "# 입력의 핏쳐가 있고, 디폴트 w(랜덤값에서 시작)에 곱셈하여 출력을 내보내도록 함 부호를 뉴런으로 생각 \n",
    "#-> ***최종적으로 도출되는 값이 신경망의 예측값임*** \n",
    "\n",
    "#전통알고리즘에서는 예측값과 정답값에서 w가 어떻게 업데이트를 하느냐. (최적화된 w를 찾는 것이 최종 목표)\n",
    "#분류군일때는 cross entropy error로 계산. 하나의 대표값이 나오는데 최소가 되어지는 값 도출해야 함\n",
    "#w는 랜덤값에서 학습이 끝나고 나면 w - 학습률 * 접선의기울기(미분구하기) => x1에 대한 x의 편미분을 구함. => 점점 정답에 가까워지게끔 업데이트가 되어질 것임\n",
    "#그래서 loss값이 최소가 되어지는 값이 학습에 대한 결과로 보여줌. \n",
    "\n",
    "#역방향을 역전파(계산하는 방법이 에러율_잔차값을 계산하는 것임) / 방향 그대로 가는 것을 순전파\n",
    "# 역전파에서 최종적으로 1의 값을 두고 봤을 때 가격에 최종 결과값을 결정하게 되는 \n",
    "# 주요포인트는 내 원래 단가에서 지불해야할 금액이 나오게 되면 가중치값에 의해서 최종단가가 결정됨\n",
    "# 최종결과는 2.2배의 결과치가 지불해야할 금액인데 1을 보면서 2.2를 계산하는게 쉽지 않음.\n",
    "# 단계를 보면서 계산을 해주게 되면 지불할 돈을 보면서 관계성을 고려하여 출력한다면 계산상 수월해짐\n",
    "\n",
    "#입력이 200인데 출력이 220이 되어야 한다고 하면 곱셈연산기는 내 입력값에 1.1을 곱해줘야하는데 그 값이 입력으로 들어가는 비중값임\n",
    "#계산기와 입력값을 핏쳐로 생각, 게이트를 생각하면 x1과 x2가 있을 때 \n",
    "\n",
    "#1차함수와 시그모이드를 통과시킨 값이 동일하다고 생각 (값이 커지면 왜곡되어지는 선형을 최소화하기 위함) => 결과적으로 로그값이 나옴\n",
    "#오차값이 적어지는 단계값을 도출하는 것이 최종적인 목표\n",
    "\n",
    "#핏쳐의 값은 고정이 되어져 있고 결과적으로 예측할 수 있는 비중에 대한 값도 고정이 되어야 한다는 의미가 포함되어져 있음\n",
    "# 그 고정된 값이 사과에 대한 예제에서는 가중치로 1.1이라고 할 수 있음\n",
    "#즉, 이에 대한 관계에서 1.1을 거꾸로 도출해내기 위해서는 학습레잇에 대해 미분을 곱해서 빼면 됨!! 미분 계산이 중요\n",
    "#입력값이 뭐가 전달되는지 알고 출력값이 뭔지 알면 얼마만큼의 비중이 나와야하는지 계산돼서 나옴 -> 이 계산은 미분이라는 것임\n",
    "\n",
    "#x의 값이 입력이 됐을 때 노드에 의해서 출력된 값을 y라고 한다면 계산그래프상에 표현되는 y = f(x)를 순정파의 개념이 된다.\n",
    "#그럴때 계산그래프의 표현법에서 실질적으로 역전파라고 할 때 국소적 미분을 이용해서 가중치에 해당하는 값을 계산할 수 있음\n",
    "#계산그래프를 이용하게 되면 입력신호에 대해서 독립적으로 계산을 수행할 수 있는 장점을 가져갈 수 있음\n",
    "\n",
    "\n",
    "#손실함수로 계산한 값이 0에 가까워지는 값을 찾아낸다면 그때 업데이트된 고유 가중치값을 고정시키겠다는 것임\n",
    "\n",
    "############################################################################################################################################\n",
    "\n",
    "# 연쇄법칙 - 역전파가 하는 일의 원리\n",
    "# 합성함수는 여러 함수로 구성된 함수를 말함.\n",
    "# 덧셈 노드의 역전파: 내부망이 +되는 연산이라면 한번 학습하고 나면 w값이 유지되는 개념이다. (바이패스 시켜주면 됨 - 통과시켜서 업데이트)\n",
    "# 곱셈 노드의 역전파: 학습이 한번 끝나서 결과가 나올때 손실함수 값은 어떻게 나오게 될 것인지\n",
    "#                     손실의 오차에 대한 결과값에서 거꾸로 계산 / 입력으로 들어오는 신호를 계산된 손실함수(순정파에서 입력되어진 값)의 결과값에 곱해주기만 하면 됨\n",
    "# 덧셈 곱셈에 개념에서 미분을 바이패스와 곱셈으로 대체한 것\n",
    "\n",
    "#영향을 미치는 정도가 가중치 -> 이를 계산하여 추출하는 것이 목적!!\n",
    "\n",
    "#오차역전파법으로 얼마만큼의 가중치로 영향을 미치는지 -> 학습을 할 때마다 업데이트되어질 것임.\n",
    "# 이때, 그 가중치를 수치로 계산했을 때 1과 입력으로 전달되어지는 값 1.1로 계산되어질 것인데 곱셈노드 역전파로 1 * 650을 하게 됨\n",
    "#       +는 바이패스로 입력으로 들어오는 1.1(가중치)이 다 통과됨\n",
    "\n",
    "#분류알고리즘에서 신경망을 구성할 때 마지막 출력단만 시그모이드를 활용하고 중간 레이어 계층들은 \n",
    "#렐로함수(0보다 클때는 입력신호를 다음레이어로 전달, 그 값이 0보다 작을 때는 아무런 값도 보내지 않음)를 활용되어지는 것이 현재 일반적임\n",
    "\n",
    "#역전파의 나눗셈: 입력값에 대해서 -가 붙으면서 제곱이 됨. \n",
    "#exp => 입력신호 곱하기 입력에 대한 미분값을 곱해주면 됨 (지수함수의 미분은 그대로 지수함수)\n",
    "\n",
    "#Affine은 신경망에서 많이 접하게 되는 용어임. 행렬의 내적을 구하는 것임. (수학에서 말하는 선형대수에서의 행렬의 내적을 말함.)\n",
    "# 신경망에서 보면 어파인은 w와 x에 대한 화살표현으로 시그마와 바이어스까지 고려한 부분임. 내적연산과 바이어스의 계산까지를 말함.\n",
    "\n",
    "#선형대수를 신경망에서 어파인이라고 부름 / 출력은 몇개를 내보내고 입력은 몇개의 뉴런으로 들여보낼 것인지를 하나의 Dense로 구성\n",
    "#예측할 때는 softmax를 따로 붙이지 않음. 예측은 판단만 하면 되기 때문에 굳이 확률적으로 계산할 필요가 없음. (실제로 서비스가 이와같이 이루어짐)\n",
    "#확률적계산은 역전파법을 통해서 w의 값을 처리하기 위함이다.\n",
    "\n",
    "#예측함수를 손실함수(에러에 대한 차를 제곱해서 평균낸 값)로 연결\n",
    "#분류는 렐루나 시그모이드를 거쳐야하므로 펼치면 로그와 같은 값이 나옴\n",
    "\n",
    "#가설함수는 w * x + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092118e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crossentropy로 그림을 그리게 되면 곡선으로 그려질 수 있음. \n",
    "# 곡선에서 처음 찾는 위치 값에서 최적의 w값을 잘못찾아 처리되어지는 문제점이 발생\n",
    "# 개선의 알고리즘이 -> 모멘텀알고리즘임(계산값에 방향성을 고려하는 것임.)\n",
    "# 방향성을 고려하자는 의미는 접선의 기울기를 그릴때 최저점인줄 알고 멈추게 되는것. \n",
    "# 이때 쇠구슬이 굴러가는 것을 생각! 내려가는 힘과 속도에 의해서 조금 더 올라간 후에 아래로 내려가도록 함.\n",
    "# 즉, 물리적으로 뭔가 속도의 값을 더 고려를 해서 최저점을 찾아가도록 한다. => 이 계산은 알고리즘이 처리해줌\n",
    "\n",
    "# 아다그레이드(AdaGrad) 알고리즘\n",
    "# 신경망에서의 경사하강법의 알고리즘을 적용할 때 우리가 사용하는 \n",
    "# 경사 알고리즘에서의 중요하게 튜닝할 수 있는 학습률(데이터에 따라 변경가능했음)이 있었음 \n",
    "#=> 항상 일정한 비율로 계산해주게 되면 w가 넓게 퍼지는 분포라고 한다면 최저점을 찾아가는 것에 오랜 학습이 걸릴 수 있고 협소하면 순식간으로 커짐\n",
    "# 처음에는 랜덤값을 잡을 때 그 잡은 랜덤한 값으로 최적값에서 가까울수도 멀수도 있음. => 멀 가능성이 높음. 이때 계산된 오차값을 보고 판단\n",
    "# ==> 이때 알고리즘이 최적값에서 멀때는 학습레잇을 크게 잡아서 학습을 시키고, 오차가 줄어들수록 학습률을 작게 가변적으로 바꾸면서 학습을 시키자는 것이다.\n",
    "# 기존 신경망보다 훨씬 좋은 효과를 받을 수 있음\n",
    "#최소점을 항상 찾아가는 것이 아님. 경사가 높을 때는 변곡점을 넘어가지 못할 수 있음. 그렇기 때문에 100% 개선되지 못할 수 있음\n",
    "\n",
    "#가중치 초기값: Xavier 초기값(활성화 함수로 sigmoid를 사용할 때), He 초기값(활성화함수로 렐루를 사용할 때)의 개념이 나옴 \n",
    "# 가중치 감소 기법: 과대적합(오버피팅)을 억제해서 범용 선능을 높이도록 하는 테크닉 / 가중치 매개변수 값이 작아지도록 학습\n",
    "#가중치의 초기값은 정규분포에 의해서 분포되어져 있는 값을 0.01배한 작은 값에 랜덤하게 데이터를 뽑아와서 학습이 진행됨\n",
    "\n",
    "#배치정규화: 학습 속도를 높임.\n",
    "\n",
    "#드롭아웃: 기존 뉴런을 통해서 학습을 시키는 방법에서 현재 입력으로 전달되는 입력데이터에 최적화 되어지는 부분에서 \n",
    "#          학습이 이루어질 때 뉴런을 랜덤하게 비활성화로 반복하면서 훈련을 시키게 되면 오버피팅이 개선이 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_cpu] *",
   "language": "python",
   "name": "conda-env-tf_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
